{
  "hidden_layers": [
    48,
    24
  ],
  "learning_rate": 3.3758533588773085e-05,
  "epochs": 2000,
  "batch_size": 18,
  "model_type": "MLP",
  "sequence_length": 10,
  "input_size": 39
}